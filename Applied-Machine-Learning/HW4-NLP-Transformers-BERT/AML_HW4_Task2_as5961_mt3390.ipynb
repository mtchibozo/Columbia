{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AML_HW4_Task2_as5961_mt3390.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQx-nKsp_gNg",
        "colab_type": "text"
      },
      "source": [
        "**Applied Machine Learning - Homework 4 - Task2**\n",
        "\n",
        "Amaury Sudrie (UNI: AS5961)\n",
        "Maxime Tchibozo (UNI: MT3390)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdvigsDt38DC",
        "colab_type": "text"
      },
      "source": [
        "Foreword: Some of the methods used in this notebook are highly computationally and memory intensive. To run this code, we used Google Colab notebooks, and we encourage you to do the same."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6E0giqS_ch8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "os.chdir(\"drive/My Drive/AML/\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1aHqdg4c_kMR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQ9ddDPb_krM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('winemag-data-130k-v2.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-r8LsuUz_mtZ",
        "colab_type": "code",
        "outputId": "3e6cb5d9-a153-4e0c-af8a-a9285aee9c11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        }
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>country</th>\n",
              "      <th>description</th>\n",
              "      <th>designation</th>\n",
              "      <th>points</th>\n",
              "      <th>price</th>\n",
              "      <th>province</th>\n",
              "      <th>region_1</th>\n",
              "      <th>region_2</th>\n",
              "      <th>taster_name</th>\n",
              "      <th>taster_twitter_handle</th>\n",
              "      <th>title</th>\n",
              "      <th>variety</th>\n",
              "      <th>winery</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Italy</td>\n",
              "      <td>Aromas include tropical fruit, broom, brimston...</td>\n",
              "      <td>Vulkà Bianco</td>\n",
              "      <td>87</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Sicily &amp; Sardinia</td>\n",
              "      <td>Etna</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Kerin O’Keefe</td>\n",
              "      <td>@kerinokeefe</td>\n",
              "      <td>Nicosia 2013 Vulkà Bianco  (Etna)</td>\n",
              "      <td>White Blend</td>\n",
              "      <td>Nicosia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Portugal</td>\n",
              "      <td>This is ripe and fruity, a wine that is smooth...</td>\n",
              "      <td>Avidagos</td>\n",
              "      <td>87</td>\n",
              "      <td>15.0</td>\n",
              "      <td>Douro</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Roger Voss</td>\n",
              "      <td>@vossroger</td>\n",
              "      <td>Quinta dos Avidagos 2011 Avidagos Red (Douro)</td>\n",
              "      <td>Portuguese Red</td>\n",
              "      <td>Quinta dos Avidagos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>US</td>\n",
              "      <td>Tart and snappy, the flavors of lime flesh and...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>87</td>\n",
              "      <td>14.0</td>\n",
              "      <td>Oregon</td>\n",
              "      <td>Willamette Valley</td>\n",
              "      <td>Willamette Valley</td>\n",
              "      <td>Paul Gregutt</td>\n",
              "      <td>@paulgwine</td>\n",
              "      <td>Rainstorm 2013 Pinot Gris (Willamette Valley)</td>\n",
              "      <td>Pinot Gris</td>\n",
              "      <td>Rainstorm</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>US</td>\n",
              "      <td>Pineapple rind, lemon pith and orange blossom ...</td>\n",
              "      <td>Reserve Late Harvest</td>\n",
              "      <td>87</td>\n",
              "      <td>13.0</td>\n",
              "      <td>Michigan</td>\n",
              "      <td>Lake Michigan Shore</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Alexander Peartree</td>\n",
              "      <td>NaN</td>\n",
              "      <td>St. Julian 2013 Reserve Late Harvest Riesling ...</td>\n",
              "      <td>Riesling</td>\n",
              "      <td>St. Julian</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>US</td>\n",
              "      <td>Much like the regular bottling from 2012, this...</td>\n",
              "      <td>Vintner's Reserve Wild Child Block</td>\n",
              "      <td>87</td>\n",
              "      <td>65.0</td>\n",
              "      <td>Oregon</td>\n",
              "      <td>Willamette Valley</td>\n",
              "      <td>Willamette Valley</td>\n",
              "      <td>Paul Gregutt</td>\n",
              "      <td>@paulgwine</td>\n",
              "      <td>Sweet Cheeks 2012 Vintner's Reserve Wild Child...</td>\n",
              "      <td>Pinot Noir</td>\n",
              "      <td>Sweet Cheeks</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0   country  ...         variety               winery\n",
              "0           0     Italy  ...     White Blend              Nicosia\n",
              "1           1  Portugal  ...  Portuguese Red  Quinta dos Avidagos\n",
              "2           2        US  ...      Pinot Gris            Rainstorm\n",
              "3           3        US  ...        Riesling           St. Julian\n",
              "4           4        US  ...      Pinot Noir         Sweet Cheeks\n",
              "\n",
              "[5 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zrb2Ztg2_w73",
        "colab_type": "text"
      },
      "source": [
        "## **Question 2.1**\n",
        "Use a pretrained word-embedding (word2vec, glove or fasttext) for featurization instead of the  bag-of-words model. Does this improve classification?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fOsJhMz3G9c",
        "colab_type": "text"
      },
      "source": [
        "We will download the Google News pre-trained model.\n",
        "\n",
        "It includes word vectors for a vocabulary of 3 million words and phrases that they trained on roughly 100 billion words from a Google News dataset. The vector length is 300 features.\n",
        "\n",
        "It can be downloaded at the following link.\n",
        "https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pD4xaGbERQZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.models import Word2Vec, KeyedVectors\n",
        "from gensim.parsing.preprocessing import remove_stopwords\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.model_selection import cross_val_score, train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.pipeline import make_pipeline\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4G9ekD3EIcO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwUip1SruyPA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(df[[\"description\", \"designation\", \"title\"]],df[\"points\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHXLmFIp6ELp",
        "colab_type": "text"
      },
      "source": [
        "We preprocess the text features to ensure uniformity as we embed the documents to the feature space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2GLoUoUFRhU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def embed(string,model):\n",
        "  #We remove stopwords \n",
        "  text = string\n",
        "  text = remove_stopwords(string)\n",
        "  text = [text]\n",
        "\n",
        "  #We tokenize the text\n",
        "  vect = CountVectorizer()\n",
        "  try:\n",
        "    vect.fit(text)\n",
        "    tokens = vect.get_feature_names()\n",
        "  except ValueError:\n",
        "    #if the vocabulary for a document is empty (perhaps because of stopword removal) we will encode it as a zero vector  \n",
        "    tokens = [] \n",
        "\n",
        "  #We embed each word using the model and compute the mean vector of the entire document\n",
        "  embedded_vect = np.zeros((300,))\n",
        "  for token in tokens:\n",
        "    try:\n",
        "      embedded_vect += model.get_vector(token)\n",
        "    except KeyError:\n",
        "      pass #words not in the Google model are encoded with a zero vector\n",
        "  if len(tokens) > 0:\n",
        "    embedded_vect /= len(tokens)\n",
        "    \n",
        "  return pd.Series(embedded_vect)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y22Q_z8HBChk",
        "colab_type": "text"
      },
      "source": [
        "Once we embed the vectors of each word of a document to the feature space, we consider the vector of the sentence to be the mean vector of all these words. \n",
        "\n",
        "This is a very simple assumption, and it loses some information relative to the ordering of the words. Two documents will be encoded by the same vector so long as they contain the same words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3pnFXc4cbmb",
        "colab_type": "text"
      },
      "source": [
        "The embed function can be applied directly to a DataFrame column to extract the 300 embedded features associated with this column. We will therefore compute 3 DataFrames of 300 features each which will represent either **description**, **designation**, and **title** in the word embedded space. \n",
        "\n",
        "We will then apply the base model from Task 1 to this new dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K122LOR9dU6G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "description_embed_train = X_train.description.fillna('missing').apply(lambda x:embed(str(x),model))\n",
        "designation_embed_train = X_train.designation.fillna('missing').apply(lambda x:embed(str(x),model))\n",
        "title_embed_train = X_train.title.fillna('missing').apply(lambda x: embed(str(x),model))\n",
        "\n",
        "description_embed_test = X_test.description.fillna('missing').apply(lambda x:embed(str(x),model))\n",
        "designation_embed_test = X_test.designation.fillna('missing').apply(lambda x:embed(str(x),model))\n",
        "title_embed_test = X_test.title.fillna('missing').apply(lambda x: embed(str(x),model))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AV8-xMB6cLDh",
        "colab_type": "code",
        "outputId": "39867d11-47c2-46b5-e0b2-ce446dfe9020",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "X1_train = pd.concat([description_embed_train,designation_embed_train,title_embed_train],axis=1,ignore_index=True)\n",
        "print(np.shape(X1_train))\n",
        "X1_test = pd.concat([description_embed_test,designation_embed_test,title_embed_test],axis=1,ignore_index=True)\n",
        "print(np.shape(X1_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(97478, 900)\n",
            "(32493, 900)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0h_BzaCXukv",
        "colab_type": "code",
        "outputId": "7ac4ae40-cd91-4173-c1dd-183ea97ffe8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "model_LR = LinearRegression()\n",
        "scores = cross_val_score(model_LR, X1_train, y_train)\n",
        "model_LR.fit(X1_train, y_train)\n",
        "print(\"Score Linear Regression\", np.mean(scores))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Score Linear Regression 0.5605135324326721\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YsdW9qxMshR",
        "colab_type": "code",
        "outputId": "e50e1782-ae01-4cde-a6cf-3781b9410e79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "print(\"Score on test\", model_LR.score(X1_test,y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Score on test 0.5628322699546746\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jh_PqMuPnica",
        "colab_type": "text"
      },
      "source": [
        "Word-embedding using Word2Vec on only **description**, **designation** and **title** gives a better model than baseline model (43% accuracy on train cross-validation and 43% on test) which used all the features except **description**, **designation** and **title**.\n",
        "\n",
        "However, it does not give a better score than dummy Bag-of-Words embedding (72% accuracy on train cross-validation and 72% on test) or Tf-Idf embedding (~77% accuracy on train cross-validation). \n",
        "\n",
        "There are a few important caveats here, which are that the Google pre-trained Word2Vec model was trained on Twitter data and might not be best adapted to wine text. A second point is that the dataset contains 900 features, since the Google pre-trained model uses an embedding space of 300 dimensions.The Bag-of-Words embedding model on the other hand has 4000 dimensions.\n",
        "\n",
        "This difference in feature space size is important particularly for the Linear Regression model, where adding non-colinear dimensions greatly improves the fit.\n",
        "\n",
        "We should also remind that we used a simple method to embed documents by taking the mean vector of all their words. Improving on this assumption by using an embedded method specialized for documents and not just words could give better results. Doc2Vec is an example of a different way to embed documents while keeping some of the structure relative to the ordering of the words.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJDmJx7Nhagj",
        "colab_type": "text"
      },
      "source": [
        "## **Question 2.1**\n",
        "\n",
        "How about combining the embedded  words with the BoW model?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqsHSU2yxOrt",
        "colab_type": "text"
      },
      "source": [
        "X_train contains the original description, designation and title features\n",
        "X1_train contains the Word2Vec encoded features.\n",
        "We concatenate the original and Word2Vec features, and OneHotEncode the original features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evUGEWwWNQGw",
        "colab_type": "code",
        "outputId": "bb691cb3-23a4-406a-9b0a-184337a7d53d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 690
        }
      },
      "source": [
        "X2_train = pd.concat([X_train,X1_train],axis=1,ignore_index=True)\n",
        "X2_train = X2_train.rename(columns={0:\"description\",1:\"designation\",2:\"title\"})\n",
        "X2_train = X2_train.fillna('missing')\n",
        "X2_train.head()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>description</th>\n",
              "      <th>designation</th>\n",
              "      <th>title</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>863</th>\n",
              "      <th>864</th>\n",
              "      <th>865</th>\n",
              "      <th>866</th>\n",
              "      <th>867</th>\n",
              "      <th>868</th>\n",
              "      <th>869</th>\n",
              "      <th>870</th>\n",
              "      <th>871</th>\n",
              "      <th>872</th>\n",
              "      <th>873</th>\n",
              "      <th>874</th>\n",
              "      <th>875</th>\n",
              "      <th>876</th>\n",
              "      <th>877</th>\n",
              "      <th>878</th>\n",
              "      <th>879</th>\n",
              "      <th>880</th>\n",
              "      <th>881</th>\n",
              "      <th>882</th>\n",
              "      <th>883</th>\n",
              "      <th>884</th>\n",
              "      <th>885</th>\n",
              "      <th>886</th>\n",
              "      <th>887</th>\n",
              "      <th>888</th>\n",
              "      <th>889</th>\n",
              "      <th>890</th>\n",
              "      <th>891</th>\n",
              "      <th>892</th>\n",
              "      <th>893</th>\n",
              "      <th>894</th>\n",
              "      <th>895</th>\n",
              "      <th>896</th>\n",
              "      <th>897</th>\n",
              "      <th>898</th>\n",
              "      <th>899</th>\n",
              "      <th>900</th>\n",
              "      <th>901</th>\n",
              "      <th>902</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>51229</th>\n",
              "      <td>Made in the popular style, this Chardonnay sho...</td>\n",
              "      <td>Light Horse</td>\n",
              "      <td>Jamieson Ranch 2012 Light Horse Chardonnay (Ca...</td>\n",
              "      <td>0.004538</td>\n",
              "      <td>0.008447</td>\n",
              "      <td>-0.034229</td>\n",
              "      <td>0.128094</td>\n",
              "      <td>0.040683</td>\n",
              "      <td>-0.029161</td>\n",
              "      <td>0.080147</td>\n",
              "      <td>-0.143331</td>\n",
              "      <td>0.079052</td>\n",
              "      <td>0.154678</td>\n",
              "      <td>0.008160</td>\n",
              "      <td>-0.136755</td>\n",
              "      <td>-0.059067</td>\n",
              "      <td>0.032295</td>\n",
              "      <td>-0.073273</td>\n",
              "      <td>0.151741</td>\n",
              "      <td>-0.011379</td>\n",
              "      <td>0.114426</td>\n",
              "      <td>-0.039024</td>\n",
              "      <td>-0.063713</td>\n",
              "      <td>-0.038864</td>\n",
              "      <td>0.175204</td>\n",
              "      <td>0.071632</td>\n",
              "      <td>-0.006744</td>\n",
              "      <td>-0.028549</td>\n",
              "      <td>-0.093174</td>\n",
              "      <td>-0.060565</td>\n",
              "      <td>0.090363</td>\n",
              "      <td>-0.047951</td>\n",
              "      <td>0.006431</td>\n",
              "      <td>-0.020082</td>\n",
              "      <td>0.071411</td>\n",
              "      <td>0.040188</td>\n",
              "      <td>0.075432</td>\n",
              "      <td>-0.040283</td>\n",
              "      <td>0.012189</td>\n",
              "      <td>0.062436</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.084124</td>\n",
              "      <td>-0.050842</td>\n",
              "      <td>0.055878</td>\n",
              "      <td>-0.000355</td>\n",
              "      <td>0.140416</td>\n",
              "      <td>-0.073469</td>\n",
              "      <td>-0.119350</td>\n",
              "      <td>-0.129953</td>\n",
              "      <td>-0.075474</td>\n",
              "      <td>0.034319</td>\n",
              "      <td>-0.010931</td>\n",
              "      <td>0.001808</td>\n",
              "      <td>-0.003069</td>\n",
              "      <td>0.068290</td>\n",
              "      <td>0.064218</td>\n",
              "      <td>0.046735</td>\n",
              "      <td>-0.036150</td>\n",
              "      <td>-0.139579</td>\n",
              "      <td>-0.068346</td>\n",
              "      <td>0.072021</td>\n",
              "      <td>0.036761</td>\n",
              "      <td>0.016593</td>\n",
              "      <td>0.125035</td>\n",
              "      <td>0.004813</td>\n",
              "      <td>-0.065360</td>\n",
              "      <td>-0.029084</td>\n",
              "      <td>-0.026751</td>\n",
              "      <td>0.042794</td>\n",
              "      <td>-0.003963</td>\n",
              "      <td>0.111921</td>\n",
              "      <td>-0.010254</td>\n",
              "      <td>0.012442</td>\n",
              "      <td>-0.006627</td>\n",
              "      <td>0.126831</td>\n",
              "      <td>0.057199</td>\n",
              "      <td>-0.026437</td>\n",
              "      <td>-0.036133</td>\n",
              "      <td>-0.014439</td>\n",
              "      <td>0.037354</td>\n",
              "      <td>-0.001011</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>128703</th>\n",
              "      <td>Really beautiful. Walks right up to the tartne...</td>\n",
              "      <td>Dutton Ranch-Sanchietti Vineyard</td>\n",
              "      <td>Dutton-Goldfield 2007 Dutton Ranch-Sanchietti ...</td>\n",
              "      <td>0.015193</td>\n",
              "      <td>0.062552</td>\n",
              "      <td>0.023916</td>\n",
              "      <td>0.111283</td>\n",
              "      <td>-0.036816</td>\n",
              "      <td>-0.009146</td>\n",
              "      <td>0.095547</td>\n",
              "      <td>-0.107268</td>\n",
              "      <td>-0.024742</td>\n",
              "      <td>0.176270</td>\n",
              "      <td>0.007625</td>\n",
              "      <td>-0.111667</td>\n",
              "      <td>-0.083593</td>\n",
              "      <td>-0.003375</td>\n",
              "      <td>-0.128272</td>\n",
              "      <td>0.077013</td>\n",
              "      <td>-0.101976</td>\n",
              "      <td>0.158099</td>\n",
              "      <td>0.003989</td>\n",
              "      <td>-0.160057</td>\n",
              "      <td>-0.071316</td>\n",
              "      <td>0.067333</td>\n",
              "      <td>0.055727</td>\n",
              "      <td>-0.041492</td>\n",
              "      <td>0.041793</td>\n",
              "      <td>-0.048252</td>\n",
              "      <td>-0.069274</td>\n",
              "      <td>0.124249</td>\n",
              "      <td>-0.063909</td>\n",
              "      <td>-0.019145</td>\n",
              "      <td>-0.025027</td>\n",
              "      <td>0.061260</td>\n",
              "      <td>0.055101</td>\n",
              "      <td>0.015242</td>\n",
              "      <td>-0.099138</td>\n",
              "      <td>-0.035468</td>\n",
              "      <td>0.005484</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.121436</td>\n",
              "      <td>-0.100146</td>\n",
              "      <td>0.020435</td>\n",
              "      <td>0.021822</td>\n",
              "      <td>0.165112</td>\n",
              "      <td>-0.002246</td>\n",
              "      <td>-0.163538</td>\n",
              "      <td>-0.118799</td>\n",
              "      <td>-0.100287</td>\n",
              "      <td>-0.081067</td>\n",
              "      <td>-0.048442</td>\n",
              "      <td>-0.004248</td>\n",
              "      <td>-0.042920</td>\n",
              "      <td>0.028760</td>\n",
              "      <td>0.102533</td>\n",
              "      <td>0.084558</td>\n",
              "      <td>0.069409</td>\n",
              "      <td>-0.122241</td>\n",
              "      <td>-0.041772</td>\n",
              "      <td>0.100562</td>\n",
              "      <td>0.036133</td>\n",
              "      <td>-0.030676</td>\n",
              "      <td>0.139648</td>\n",
              "      <td>-0.068652</td>\n",
              "      <td>0.035107</td>\n",
              "      <td>-0.019983</td>\n",
              "      <td>-0.054990</td>\n",
              "      <td>0.048804</td>\n",
              "      <td>-0.089868</td>\n",
              "      <td>0.178711</td>\n",
              "      <td>-0.074121</td>\n",
              "      <td>-0.044608</td>\n",
              "      <td>-0.067029</td>\n",
              "      <td>0.087268</td>\n",
              "      <td>0.056378</td>\n",
              "      <td>-0.102100</td>\n",
              "      <td>-0.010669</td>\n",
              "      <td>-0.032373</td>\n",
              "      <td>0.130957</td>\n",
              "      <td>0.079175</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26320</th>\n",
              "      <td>Fresh-cut grass and gooseberry dominate on the...</td>\n",
              "      <td>missing</td>\n",
              "      <td>Apriori 2014 Sauvignon Blanc (Napa Valley)</td>\n",
              "      <td>0.035358</td>\n",
              "      <td>0.047158</td>\n",
              "      <td>0.029969</td>\n",
              "      <td>0.058912</td>\n",
              "      <td>-0.053372</td>\n",
              "      <td>-0.023125</td>\n",
              "      <td>0.029922</td>\n",
              "      <td>-0.116879</td>\n",
              "      <td>0.043968</td>\n",
              "      <td>0.212850</td>\n",
              "      <td>-0.048026</td>\n",
              "      <td>-0.147433</td>\n",
              "      <td>-0.035764</td>\n",
              "      <td>0.023300</td>\n",
              "      <td>-0.121549</td>\n",
              "      <td>0.077498</td>\n",
              "      <td>-0.003098</td>\n",
              "      <td>0.150106</td>\n",
              "      <td>0.010394</td>\n",
              "      <td>-0.055104</td>\n",
              "      <td>0.011552</td>\n",
              "      <td>0.115344</td>\n",
              "      <td>0.079547</td>\n",
              "      <td>-0.048332</td>\n",
              "      <td>0.002214</td>\n",
              "      <td>-0.043152</td>\n",
              "      <td>-0.080736</td>\n",
              "      <td>0.112940</td>\n",
              "      <td>0.034655</td>\n",
              "      <td>0.053327</td>\n",
              "      <td>-0.015981</td>\n",
              "      <td>0.013526</td>\n",
              "      <td>0.011946</td>\n",
              "      <td>-0.000834</td>\n",
              "      <td>-0.001953</td>\n",
              "      <td>-0.051216</td>\n",
              "      <td>0.011668</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.049723</td>\n",
              "      <td>-0.003988</td>\n",
              "      <td>0.012939</td>\n",
              "      <td>0.007487</td>\n",
              "      <td>0.036791</td>\n",
              "      <td>-0.051188</td>\n",
              "      <td>-0.150736</td>\n",
              "      <td>-0.055155</td>\n",
              "      <td>-0.099681</td>\n",
              "      <td>-0.112528</td>\n",
              "      <td>-0.130371</td>\n",
              "      <td>-0.054240</td>\n",
              "      <td>-0.057373</td>\n",
              "      <td>0.059408</td>\n",
              "      <td>0.153727</td>\n",
              "      <td>0.037354</td>\n",
              "      <td>-0.031209</td>\n",
              "      <td>-0.096649</td>\n",
              "      <td>-0.135050</td>\n",
              "      <td>0.114624</td>\n",
              "      <td>-0.134481</td>\n",
              "      <td>-0.001912</td>\n",
              "      <td>0.149902</td>\n",
              "      <td>-0.019613</td>\n",
              "      <td>0.080302</td>\n",
              "      <td>0.023275</td>\n",
              "      <td>-0.043416</td>\n",
              "      <td>0.009460</td>\n",
              "      <td>-0.110921</td>\n",
              "      <td>0.144938</td>\n",
              "      <td>0.066650</td>\n",
              "      <td>-0.065069</td>\n",
              "      <td>0.021729</td>\n",
              "      <td>0.113057</td>\n",
              "      <td>0.001506</td>\n",
              "      <td>-0.049845</td>\n",
              "      <td>-0.009766</td>\n",
              "      <td>-0.102132</td>\n",
              "      <td>0.099447</td>\n",
              "      <td>0.098226</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86520</th>\n",
              "      <td>Soft, sweet and robust, with upfront cherry pi...</td>\n",
              "      <td>missing</td>\n",
              "      <td>Greg Norman California Estates 2011 Cabernet S...</td>\n",
              "      <td>0.006704</td>\n",
              "      <td>0.004667</td>\n",
              "      <td>-0.001961</td>\n",
              "      <td>0.123652</td>\n",
              "      <td>0.032538</td>\n",
              "      <td>-0.052241</td>\n",
              "      <td>0.104507</td>\n",
              "      <td>-0.082836</td>\n",
              "      <td>0.010951</td>\n",
              "      <td>0.208471</td>\n",
              "      <td>-0.034405</td>\n",
              "      <td>-0.082240</td>\n",
              "      <td>-0.051816</td>\n",
              "      <td>-0.031211</td>\n",
              "      <td>-0.155997</td>\n",
              "      <td>0.128997</td>\n",
              "      <td>-0.073486</td>\n",
              "      <td>0.110730</td>\n",
              "      <td>-0.090595</td>\n",
              "      <td>-0.129211</td>\n",
              "      <td>-0.119261</td>\n",
              "      <td>0.053972</td>\n",
              "      <td>0.099284</td>\n",
              "      <td>-0.037384</td>\n",
              "      <td>0.033061</td>\n",
              "      <td>-0.065063</td>\n",
              "      <td>-0.106211</td>\n",
              "      <td>0.111762</td>\n",
              "      <td>-0.057752</td>\n",
              "      <td>-0.006688</td>\n",
              "      <td>-0.012605</td>\n",
              "      <td>0.037445</td>\n",
              "      <td>0.055527</td>\n",
              "      <td>-0.022207</td>\n",
              "      <td>-0.090266</td>\n",
              "      <td>-0.027528</td>\n",
              "      <td>0.072769</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.025608</td>\n",
              "      <td>-0.012899</td>\n",
              "      <td>0.082140</td>\n",
              "      <td>-0.048991</td>\n",
              "      <td>-0.040827</td>\n",
              "      <td>-0.093316</td>\n",
              "      <td>-0.125956</td>\n",
              "      <td>-0.044542</td>\n",
              "      <td>-0.006402</td>\n",
              "      <td>-0.134820</td>\n",
              "      <td>-0.120524</td>\n",
              "      <td>0.036458</td>\n",
              "      <td>-0.007582</td>\n",
              "      <td>0.138563</td>\n",
              "      <td>0.088881</td>\n",
              "      <td>0.047404</td>\n",
              "      <td>-0.050646</td>\n",
              "      <td>-0.044881</td>\n",
              "      <td>-0.105876</td>\n",
              "      <td>0.167508</td>\n",
              "      <td>-0.017666</td>\n",
              "      <td>0.020725</td>\n",
              "      <td>0.154894</td>\n",
              "      <td>-0.041930</td>\n",
              "      <td>0.057590</td>\n",
              "      <td>-0.018656</td>\n",
              "      <td>-0.073676</td>\n",
              "      <td>0.003916</td>\n",
              "      <td>-0.033088</td>\n",
              "      <td>0.104709</td>\n",
              "      <td>0.116374</td>\n",
              "      <td>-0.012207</td>\n",
              "      <td>0.043457</td>\n",
              "      <td>0.091390</td>\n",
              "      <td>0.004293</td>\n",
              "      <td>-0.116333</td>\n",
              "      <td>-0.045844</td>\n",
              "      <td>-0.130046</td>\n",
              "      <td>0.036187</td>\n",
              "      <td>0.089949</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>103889</th>\n",
              "      <td>This is an ebullient wine, full of herb, grape...</td>\n",
              "      <td>missing</td>\n",
              "      <td>Lava Cap 2016 Sauvignon Blanc (El Dorado)</td>\n",
              "      <td>0.030413</td>\n",
              "      <td>-0.000655</td>\n",
              "      <td>0.031645</td>\n",
              "      <td>0.097782</td>\n",
              "      <td>0.008959</td>\n",
              "      <td>-0.029075</td>\n",
              "      <td>0.025143</td>\n",
              "      <td>-0.085933</td>\n",
              "      <td>0.006450</td>\n",
              "      <td>0.167956</td>\n",
              "      <td>-0.071790</td>\n",
              "      <td>-0.111694</td>\n",
              "      <td>-0.050964</td>\n",
              "      <td>0.031629</td>\n",
              "      <td>-0.108321</td>\n",
              "      <td>0.182315</td>\n",
              "      <td>-0.040001</td>\n",
              "      <td>0.241294</td>\n",
              "      <td>0.071578</td>\n",
              "      <td>-0.080978</td>\n",
              "      <td>-0.014779</td>\n",
              "      <td>0.123214</td>\n",
              "      <td>0.064564</td>\n",
              "      <td>-0.063891</td>\n",
              "      <td>0.018709</td>\n",
              "      <td>-0.093374</td>\n",
              "      <td>-0.039122</td>\n",
              "      <td>0.113301</td>\n",
              "      <td>-0.071429</td>\n",
              "      <td>0.026878</td>\n",
              "      <td>-0.033732</td>\n",
              "      <td>0.030345</td>\n",
              "      <td>0.045631</td>\n",
              "      <td>0.022943</td>\n",
              "      <td>-0.093063</td>\n",
              "      <td>-0.019426</td>\n",
              "      <td>-0.003678</td>\n",
              "      <td>...</td>\n",
              "      <td>0.059396</td>\n",
              "      <td>0.067836</td>\n",
              "      <td>-0.006383</td>\n",
              "      <td>-0.005136</td>\n",
              "      <td>-0.056608</td>\n",
              "      <td>-0.046392</td>\n",
              "      <td>-0.153041</td>\n",
              "      <td>-0.077515</td>\n",
              "      <td>-0.003627</td>\n",
              "      <td>-0.173619</td>\n",
              "      <td>-0.172799</td>\n",
              "      <td>-0.030169</td>\n",
              "      <td>-0.005511</td>\n",
              "      <td>0.049395</td>\n",
              "      <td>0.172355</td>\n",
              "      <td>0.174159</td>\n",
              "      <td>-0.184640</td>\n",
              "      <td>-0.128880</td>\n",
              "      <td>-0.062919</td>\n",
              "      <td>0.008048</td>\n",
              "      <td>-0.112113</td>\n",
              "      <td>0.092041</td>\n",
              "      <td>0.113281</td>\n",
              "      <td>-0.034180</td>\n",
              "      <td>0.043326</td>\n",
              "      <td>-0.059047</td>\n",
              "      <td>-0.020752</td>\n",
              "      <td>0.041574</td>\n",
              "      <td>-0.028338</td>\n",
              "      <td>0.089704</td>\n",
              "      <td>0.099854</td>\n",
              "      <td>-0.108538</td>\n",
              "      <td>0.011719</td>\n",
              "      <td>0.025426</td>\n",
              "      <td>-0.010393</td>\n",
              "      <td>0.069188</td>\n",
              "      <td>0.106445</td>\n",
              "      <td>-0.044085</td>\n",
              "      <td>0.117676</td>\n",
              "      <td>0.163644</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 903 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              description  ...       902\n",
              "51229   Made in the popular style, this Chardonnay sho...  ... -0.001011\n",
              "128703  Really beautiful. Walks right up to the tartne...  ...  0.079175\n",
              "26320   Fresh-cut grass and gooseberry dominate on the...  ...  0.098226\n",
              "86520   Soft, sweet and robust, with upfront cherry pi...  ...  0.089949\n",
              "103889  This is an ebullient wine, full of herb, grape...  ...  0.163644\n",
              "\n",
              "[5 rows x 903 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sf74jtIM_oGH",
        "colab_type": "code",
        "outputId": "73eb2f2d-8b43-41b3-ea9d-4cc77372a46a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "tfidf1 = make_pipeline(CountVectorizer(min_df=2, max_features=2000), TfidfTransformer())\n",
        "tfidf2 = make_pipeline(CountVectorizer(min_df=2, max_features=1000, max_df=0.95), TfidfTransformer())\n",
        "tfidf3 = make_pipeline(CountVectorizer(min_df=2, max_features=1000, max_df=0.95), TfidfTransformer())\n",
        "\n",
        "BoW_train = sp.sparse.hstack([\n",
        "    tfidf1.fit_transform(X2_train[\"description\"]),\n",
        "    tfidf2.fit_transform(X2_train[\"designation\"]),\n",
        "    tfidf3.fit_transform(X2_train[\"title\"]),\n",
        "    description_embed_train,\n",
        "    designation_embed_train,\n",
        "    title_embed_train\n",
        "])\n",
        "\n",
        "BoW_train"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<97478x4900 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 86834902 stored elements in COOrdinate format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZxRF8Ie1VkM",
        "colab_type": "text"
      },
      "source": [
        "This sparse matrix contains 86 million elements, which is mind-blowingly large. For this reason, the following Linear Regression code takes a long time to execute."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDWrf8-bA8iw",
        "colab_type": "code",
        "outputId": "073236db-22ef-4cfb-c02e-e7d52a7f40b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "LR = LinearRegression()\n",
        "scores = cross_val_score(LR, BoW_train, y_train)\n",
        "\n",
        "print(\"Score Linear Regression\", np.mean(scores))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Score Linear Regression 0.7374600040355238\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHZjwDvA5IvD",
        "colab_type": "text"
      },
      "source": [
        "Using both  Word2Vec embedding and the Bag of Words model yields a better score than simple Bag of words. However, this score is only a few percentage points better than simple Bag of words with Tf-idf yet much more computationally intensive. This method could be useful if we are interested in having the best possible score no matter the computational cost.\n",
        "\n",
        "For general applications, however, it is more efficient to use Bag of Words with Tf-idF and the non-text features than Tf-idf and word2vec features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zs9sQ0aJDOFz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}